{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_folder =  (\"C:/Users/pc/Desktop/train_sample_videos\")\n",
    "test_folder =  (\"C:/Users/pc/Desktop/test_videos\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_files  = os.listdir(train_sample_folder)\n",
    "train_sample_file_count = len(train_sample_files)\n",
    "print(f\"Number of files in train sample folder: {train_sample_file_count}\")\n",
    "\n",
    "test_files = os.listdir(test_folder)\n",
    "test_file_count = len(test_files)\n",
    "print(f\"Number of files in test folder: {test_file_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_json(\"C:/Users/pc/Desktop/meta/metadata.json\").T\n",
    "train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.groupby('label')['label'].count().plot(figsize=(3,3),kind='bar',title='Training Set Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain_sample_video = list(train_metadata.loc[train_metadata.label=='FAKE'].sample(7).index)\n",
    "ftrain_sample_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_image_from_video(video_path):\n",
    "    capture_image = cv2.VideoCapture(video_path)\n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize =(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in ftrain_sample_video:\n",
    "    capture_image_from_video(os.path.join(train_sample_folder, video_file))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAKE VIDEO FILES\n",
    "f_videos = list(train_metadata.loc[train_metadata.label=='FAKE'].index)\n",
    "f_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBER OF FAKE VIDEOS\n",
    "f_videos = len(list(train_metadata.loc[train_metadata.label=='FAKE'].index))\n",
    "f_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REAL VIDEO FILES\n",
    "R_videos = list(train_metadata.loc[train_metadata.label=='REAL'].index)\n",
    "R_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBER OF REAL VIDEOS\n",
    "R_videos = len(list(train_metadata.loc[train_metadata.label=='REAL'].index))\n",
    "R_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtrain_sample_video = list(train_metadata.loc[train_metadata.label=='REAL'].sample(3).index)\n",
    "Rtrain_sample_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in Rtrain_sample_video:\n",
    "    capture_image_from_video(os.path.join(train_sample_folder,video_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play_video(video_file):\n",
    "    video_url = open(video_file, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" %data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To specify a particular file\n",
    "play_video(\"C:/Users/pc/Desktop/train_sample_videos/aapnvogymq.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(os.path.join(train_sample_folder, train_sample_files[100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=10):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Compute the frame indices to extract\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "\n",
    "    # Initialize an empty list to store the frames\n",
    "    frames = []\n",
    "\n",
    "    # Loop through the selected frame indices and extract the frames\n",
    "    for index in frame_indices:\n",
    "        # Set the frame index\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "\n",
    "        # Read the frame from the video file\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Add the frame to the list\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Release the video file\n",
    "    cap.release()\n",
    "\n",
    "    # Return the list of frames as a NumPy array\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(train_sample_folder, train_sample_files[0])\n",
    "frames = extract_frames(video_path, num_frames=10)\n",
    "frames"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
