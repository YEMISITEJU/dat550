Convolutional Neural Networks (CNNs) are a type of deep learning
algorithm that have been widely used for image and video process-
ing tasks. CNNs can be used for deepfake detection because they
are able to automatically learn and extract relevant features from
images and videos. There are several different approaches to using
CNNs for deepfake detection. In this project I try to use CNN to do
frame-level analysis which involves analyzing individual frames
captured from deepfake videos and real videos to detect inconsis-
tencies and artifacts that may indicate a deepfake. To train a CNN
for deepfake detection, a large dataset of both real and fake images
or videos is required. The network is then trained using a process
called backpropagation, which involves iteratively adjusting the
weights of the network to minimize a loss function that measures
the difference between the predicted outputs and the true labels of
the data. The training process typically involves multiple epochs,
where the entire dataset is fed through the network multiple times,
and can take several hours or days depending on the size of the
dataset and the complexity of the network architecture.
The architecture of the CNN I use for deepfake detection consists
of multiple layers.
• Rescaling layer: This layer scales the input values of the
image pixels by dividing them by 255. This is done to nor-
malize the pixel values in the range of 0 to 1, which is
usually helpful for training machine learning models.
• Conv2D layer: This layer performs convolution operation
on the input image. It uses 32 filters of size 3x3 to extract
features from the image. The activation function used is
’relu’, which helps to introduce non-linearity in the output.
• MaxPooling2D layer: This layer performs max pooling op-
eration on the output of the previous layer. It reduces the
size of the feature maps by selecting the maximum value
within a window of size 2x2. This helps to reduce the com-
putational cost and also prevent overfitting.
• Flatten layer: This layer flattens the output of the previous
layer into a 1D array. This is done to pass the data into the
next fully connected layer.
• Dense layer: This layer is a fully connected layer with 128
neurons. It applies the activation function ’relu’ to introduce
non-linearity in the output.
• Dense layer: This layer is the output layer of the model with
a single neuron. It applies the activation function ’sigmoid’
to get the probability of the input image belonging to a
particular class.
When the model is compiled, ’adam’ optimizer is being used,
which is a popular optimization algorithm for deep learning models.
The Binary Crossentropy loss function is being used, which is a
suitable loss function for binary classification problems. Metrics
parameter is used to specify the evaluation metrics that the model
will use to monitor its performance during training and testing.
The model is being evaluated on three metrics - accuracy, precision,
and recall. Accuracy measures the percentage of correctly classified
samples. Precision measures the proportion of true positive predic-
tions out of all positive predictions. Recall measures the proportion
of true positive predictions out of all actual positive samples.

Due to limited computational resources, I used the videos in dfdc
train part 10 folder from the Deepfake Detection Challenge (DFDC)
dataset to train the CNN model. The folder contains 3192 videos
of which 2813 are deepfake videos. I captured 10 frames from each
video and then extracted faces from each frame using MTCNN from
facenet_pytorch package. The extracted faces of training videos
are saved in JPEG format and into two folders called 0_REAL_faces
and 1_FAKE_faces.
Images in 0_REAL_faces and 1_FAKE_faces folders are loaded
using tf.keras.utils.image_dataset_from_directory into two sepa-
rate classes representing real and fake face images, splitting 70%
to training set and 30% to validation set. Another smaller DFDC
dataset, which consists of 400 videos of which 323 are deepfake
videos, is downloaded and used as a test dataset for the CNN model.
I have captured 10 frames from each video in the test dataset and
extracted faces to JPEG images in the same way as the training
dataset.
When the CNN model is fitted on the training dataset in 3 epochs
with a batch size of 32, it achieves an accuracy of 0.9233 and a loss
of 0.2168. The CNN model is evaluated on the test dataset and gets
an accuracy of 0.7546 and with a loss of 0.6070.
When the CNN model is fitted on the training dataset in 2 epochs
with a batch size of 32, it achieves an accuracy of 0.9126 and a loss
of 0.2417. The CNN model is evaluated on the test dataset and gets
an accuracy of 0.7632 and with a loss of 0.5534.
