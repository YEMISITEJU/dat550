{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset from train sample videos json file\n",
    "def load_json_into_df(file:str):\n",
    "    files = []\n",
    "    file_path = f\"../../train_sample_videos/{file}\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        for x in data:\n",
    "            files.append({\n",
    "                \"name\":x,\n",
    "                \"label\":data[x][\"label\"],\n",
    "                \"split\":data[x][\"split\"],\n",
    "                \"original\":data[x][\"original\"]\n",
    "            })\n",
    "    return pd.DataFrame(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_df = load_json_into_df(\"metadata.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "             name label  split        original\n0  aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n1  aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n2  abarnvbtwb.mp4  REAL  train            None\n3  abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n4  abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>label</th>\n      <th>split</th>\n      <th>original</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aagfhgtpmv.mp4</td>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>vudstovrck.mp4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aapnvogymq.mp4</td>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>jdubbvfswz.mp4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abarnvbtwb.mp4</td>\n      <td>REAL</td>\n      <td>train</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abofeumbvv.mp4</td>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>atvmxvwyns.mp4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abqwwspghj.mp4</td>\n      <td>FAKE</td>\n      <td>train</td>\n      <td>qzimuostzz.mp4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def extract_frames(video_path:str, num_frames:int=10):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Get the total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Determine no of frames to extract\n",
    "    num_frames = total_frames if num_frames > total_frames else num_frames\n",
    "\n",
    "\n",
    "    # Compute the frame indices to extract\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store the frames\n",
    "    frames = []\n",
    "\n",
    "\n",
    "    # Loop through the selected frame indices and extract the frames\n",
    "    for index in frame_indices:\n",
    "        # Set the frame index\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "\n",
    "        # Read the frame from the video file\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Add the frame to the list\n",
    "        frames.append(frame)\n",
    "\n",
    "\n",
    "    # Release the video file\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "    # Return the list of frames as a NumPy array\n",
    "    return np.array(frames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def extract_faces(frame):\n",
    "    face_cascade = cv2.CascadeClassifier('../../models/haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        frame,\n",
    "        scaleFactor= 1.1,\n",
    "        minNeighbors= 20,\n",
    "        minSize=(10, 10)\n",
    "    )\n",
    "    faces_detected = format(len(faces)) + \" faces detected!\"\n",
    "    print(faces_detected)\n",
    "    # Draw a rectangle around the faces\n",
    "    faces_frames = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        faces_frames.append(face)\n",
    "    return faces_frames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def extract_frames_faces(file_name:str):\n",
    "    video_path = f\"../../train_sample_videos/{file_name}\"\n",
    "    fms = extract_frames(video_path)\n",
    "    faces_list = []\n",
    "    for fm in fms:\n",
    "        faces = extract_faces(fm)\n",
    "        faces_list.append(faces)\n",
    "    return np.array(faces_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "2 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/vbtrq_6d7tg784xl1j7tl8z00000gn/T/ipykernel_20841/567827651.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(faces_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 faces detected!\n",
      "0 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "0 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "0 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "0 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n",
      "1 faces detected!\n"
     ]
    }
   ],
   "source": [
    "train_df[\"faces\"] = train_df[\"name\"].apply(lambda x: extract_frames_faces(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}